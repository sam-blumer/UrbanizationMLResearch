---
title: "Food Desert Prediction | Practicum I"
author: "Sam Blumer"
date: "May 27, 2019"
output:
  word_document: default
  html_document: default
---

#Causes, Effects, and Predictions of Food Desert Occurence Relative to Urbanization in the United States.

According to American Nutrition Association, "Food deserts are defined as parts of the country vapid of fresh fruit, vegetables, and other healthful whole foods, usually found in impoverished areas. This is largely due to a lack of grocery stores, farmers' markets, and healthy food providers.(http://americannutritionassociation.org/newsletter/usda-defines-food-deserts)".
Over the past several decades, as urbanizationhas increased, food deserts in rural areas have also increased (https://www.iatp.org/sites/default/files/258_2_98043.pdf). In fact, 

Rural areas risk becoming "food deserts" as young families move away and market
pressures continue to squeeze small grocers and retailers. Food deserts are defined
as counties in which all residents must drive more than 10 miles to the nearest
supermarket chain or supercenter.
. The Great Plains are especially lacking in easy-access grocers.
. The residents of food deserts tend to be older, poorer, and less educated.
. Health can be compromised by lack of food access. Many do not consume adequate
amounts of fresh fruits or vegetables, and they often lack adequate dairy and
protein in their diet. 

This has become a big problem because while food deserts are often short on whole food providers, especially fresh fruits and vegetables, instead, they are heavy on local quickie marts that provide a wealth of processed, sugar, and fat laden foods that are known contributors to our nation's obesity epidemic.

With so many indicators available for analysis, we are going to attempt to use these indicators to predict counties in which a food desert may exist in the future. 


We are going to be using a variety of packages that will allow us to process, explore, analyze, and ultimately offer tools for feature engineering and machine learning. Note that we will also be using a local Spark instance for processing and storing large amounts of our data.  
  
```{r echo=FALSE, warning=FALSE}
#INSTALL PACKAGES
install.packages("sparklyr",repos = c("http://rstudio.org/_packages", "http://cran.rstudio.com"))
install.packages("dbplyr",repos = c("http://rstudio.org/_packages", "http://cran.rstudio.com"))
install.packages("DBI",repos = c("http://rstudio.org/_packages", "http://cran.rstudio.com"))
install.packages("standardize",repos = c("http://rstudio.org/_packages", "http://cran.rstudio.com"))
install.packages("Matrix",repos = c("http://rstudio.org/_packages", "http://cran.rstudio.com"))
install.packages("psycho",repos = c("http://rstudio.org/_packages", "http://cran.rstudio.com"), dependencies = TRUE)
install.packages("shiny",repos = c("http://rstudio.org/_packages", "http://cran.rstudio.com"))
install.packages("reshape2",repos = c("http://rstudio.org/_packages", "http://cran.rstudio.com"), dependencies = TRUE)
install.packages("grid",repos = c("http://rstudio.org/_packages", "http://cran.rstudio.com"), dependencies = TRUE))
library(grid)
library(gridExtra)
library(shiny)
library(standardize)
library(psycho)
library(tidyverse)
library(sparklyr)
library(caret)
library(dplyr)
library(dbplyr)
library(DBI)
library(ggplot2)
library(randomForest)
library(naniar)
library(reshape2)
```


For our local Spark instance, we are going to use Spark 2.1.0 and set-up particular configuration specifcations based off the computer we are using for our analysis. As the code below notes, are are using 2 cores and 8GB of local RAM to store and process our data via our Spark cluster.
  
```{r}
spark_install(version = "2.1.0")

conf <- spark_config()
conf$`sparklyr.cores.local` <- 2
conf$`sparklyr.shell.driver-memory` <- "8G"
conf$spark.memory.fraction <- 0.9
```

With our configuration created, we can start our local spark cluster directly via our code as opposed to the R user interface.  
```{r}
sc <- spark_connect(master = "local", version = "2.1.0", config = conf)
```

#Load Data

We have multiple data sources that we are going to inspect and aggregate to make a more complete data set for our final evaluation. Because we have so much data, we are going to use Spark to store and distribute the processing. Files were stored locally and will be provided as a part of the final report.

```{r}
access2010 <- spark_read_csv(sc, name = 'access2010', path = 'C:/Users/blume/Desktop/practicum/data/2010/access2010.csv', header = TRUE, delimeter = ',', stringsAsFactors = FALSE)
assistance2010 <- spark_read_csv(sc, name = 'assistance2010', path = 'C:/Users/blume/Desktop/practicum/data/2010/assistance2010.csv', header = TRUE, delimeter = ',', stringsAsFactors = FALSE)
health2010 <- spark_read_csv(sc, name = 'health2010', path = 'C:/Users/blume/Desktop/practicum/data/2010/health2010.csv', header = TRUE, delimeter = ',', stringsAsFactors = FALSE)
insecurity2010 <- spark_read_csv(sc, name = 'insecurity2010', path = 'C:/Users/blume/Desktop/practicum/data/2010/insecurity2010.csv', header = TRUE, delimeter = ',', stringsAsFactors = FALSE)
local2010 <- spark_read_csv(sc, name = 'local2010', path = 'C:/Users/blume/Desktop/practicum/data/2010/local2010.csv', header = TRUE, delimeter = ',', stringsAsFactors = FALSE)
restaurants2010 <- spark_read_csv(sc, name = 'restaurants2010', path = 'C:/Users/blume/Desktop/practicum/data/2010/restaurants2010.csv', header = TRUE, delimeter = ',', stringsAsFactors = FALSE)
socioeconomic2010 <- spark_read_csv(sc, name = 'socioeconomic2010', path = 'C:/Users/blume/Desktop/practicum/data/2010/socioeconomic2010.csv', header = TRUE, delimeter = ',', stringsAsFactors = FALSE)
store2010 <- spark_read_csv(sc, name = 'store2010', path = 'C:/Users/blume/Desktop/practicum/data/2010/store2010.csv', header = TRUE, delimeter = ',', stringsAsFactors = FALSE)
populations <- spark_read_csv(sc, name = 'populations', path = 'C:/Users/blume/Desktop/practicum/data/county_populations.csv', header = TRUE, delimeter = ',', stringsAsFactors = FALSE)

```

#loading this file into R directly
This file is sourced from XXX and gives counties that contained food deserts in the year 2011. Because of the file size, we can source it directly to our local instance of R and use it as ourresponse variable. 
```{r}
food_desert <- read.csv("C:/Users/blume/Desktop/practicum/data/fooddesert.csv", header = TRUE, sep = ",", stringsAsFactors = FALSE)
pops <- read.csv('C:/Users/blume/Desktop/practicum/data/county_populations.csv', header = TRUE, sep = ',', stringsAsFactors = FALSE)
county_char <- read.csv("C:/Users/blume/Desktop/practicum/data/county_chars.csv", header = TRUE, sep = ',', stringsAsFactors = FALSE)

colnames(food_desert)[colnames(food_desert)=="ï..State"] <- "State"
colnames(county_char)[colnames(county_char)=="ï..FIPS"] <- "FIPS"
food_desert_factor <- food_desert %>% select(State, County, Desert)

names(pops) <- c("State", "County", "pop2010", "pop2011", "pop2012", "pop2013", "pop2014", "pop2015", "pop2016", "pop2017", "pop2018")
county_char$Continuum_Description <- NULL


```
#
#
#
#

#DATA PRE-PROCESSING

The data as it was provided is in separate CSV files, these files were loaded individually into the local Spark cluster, but we will want to merge them into a flat file, where each row represents one US county, and each column is a descriptor of that county.

```{r}
output2010 <- Reduce(function(...) merge(..., all=TRUE), list(access2010, assistance2010, health2010, insecurity2010, local2010, restaurants2010, socioeconomic2010, store2010))
```

Variables in the file need to be updated, including updating character and factor columns. The associated data dictionary describes in each column and data in type in depth.


``````{r}
#output2010$MEDHHINC <- as.numeric(output2010$MEDHHINC)
#output2010$CHILDPOVRATE <- as.numeric(output2010$CHILDPOVRATE)
#output2010$POVRATE <- as.numeric(output2010$POVRATE)

output2010$FIPS <- as.character(output2010$FIPS)
output2010$METRO <- as.factor(output2010$METRO)
output2010$SNAP_OAPP <- as.factor(output2010$SNAP_OAPP)
output2010$SNAP_FACEWAIVER <- as.factor(output2010$SNAP_FACEWAIVER)
output2010$SNAP_VEHEXCL <- as.factor(output2010$SNAP_VEHEXCL)
output2010$SNAP_BBCE <- as.factor(output2010$SNAP_BBCE)
output2010$SNAP_REPORTSIMPLE <- as.factor(output2010$SNAP_REPORTSIMPLE)
output2010$FOODHUB <- as.factor(output2010$FOODHUB)
output2010$FARM_TO_SCHOOL <- as.factor(output2010$FARM_TO_SCHOOL)
output2010$PERPOV <- as.factor(output2010$PERPOV)
output2010$PERCHLDPOV <- as.factor(output2010$PERCHLDPOV)
```

The files contain counties from Puerto Rico, which will not be used in our analysis. We can quickly identify those and remove them using the subset() function:

```{r}
output2010 <- subset(output2010, nchar(as.character(State)) <= 2)
```

##MISSING DATA
XXXXXXAs a part of our merge, we joined columns where there were 

```{r}
output2010[is.na(output2010)] <- 0

output2010 <- na.omit(output2010)
```

With our first file flattened and cleaned, we now want to merge it with our food desert file that contains our response variable. Again, we will use the merge function and merge the rows based on matching state and county.
```{r}
outfd <- merge(output2010, food_desert_factor, by = c("State", "County"), all.x = TRUE)
```

The merge is essentiall a left outer join on our first file, and the food desert file only contains values for counties that included food deserts, therefore, any county from our flattened file that did not have a value in the food desert file will have an NA. Since we simply want to use this as a bivariate response column, we can change the NA values to 0s, then change the data type.

```{r}
outfd$Desert[is.na(outfd$Desert)] <- 0

#convert our response variable to a factor
outfd$Desert <- as.factor(outfd$Desert)
outfd$MEDHHINC <- as.numeric(outfd$MEDHHINC)
outfd$CHILDPOVRATE <- as.numeric(outfd$CHILDPOVRATE)
outfd$POVRATE <- as.numeric(outfd$POVRATE)

```

Confirm we are not missing any data:

```{r}
vis_miss(outfd)
final <- na.omit(outfd)
```


















#DATA EXPLORATION
###With the file cleaned and assembled, we can begin to explre the data itself.

```{r}

county_char$Metro2013[county_char$Metro2013 == 1] <- "Urban"
county_char$Metro2013[county_char$Metro2013 == 0] <- "Rural"

pops_type <- merge(pops, county_char)
#nums <- c(3:10)
#pops_type[,nums] <- lapply(pops_type[,nums] , integer)

series <- pops_type %>% group_by(Metro2013) %>% 
  summarise_at(3:11, sum)


urban_series <- pops_type %>% group_by(Metro2013) %>% 
  filter(Metro2013 == "Urban") %>% 
  summarise_at(3:11, sum)

rural_series <- pops_type %>% group_by(Metro2013) %>% 
    filter(Metro2013 == "Rural") %>% 
    summarise_at(3:11, sum)

rural <- melt(rural_series)
urban <- melt(urban_series)

rts <- ts(rural$value, start=c(2010), end=c(2018), frequency=1)  
uts <- ts(urban$value, start=c(2010), end=c(2018), frequency=1)


rts_plot <- plot((rts/1000000), main="Rural Populations Over Time", sub="United States 2010-2018",
     xlab="Date", ylab="Population in Millions")
     #xlim=c(xmin, xmax), ylim=c(ymin, ymax)) 


uts_plot <- plot((uts/1000000), main="Urban Populations Over Time", sub="United States 2010-2018",
     xlab="Date", ylab="Population in Millions")
     #xlim=c(xmin, xmax), ylim=c(ymin, ymax)) 

meltdf <- melt(series)

meltts <- ggplot(meltdf) +
  # add bar for each discipline colored by gender
  geom_bar(aes(x = str_remove(variable, "pop"), y = (value/1000000), fill = Metro2013),
           stat = "identity", position = "dodge") +
  # name axes and remove gap between bars and y-axis
  #scale_y_continuous("Value", expand = c(0, 0)) +
  #scale_x_discrete("variable") +
  scale_fill_manual(values = c("#468189", "#9DBEBB")) +
  # remove grey theme
  theme_classic(base_size = 18) +
  # rotate x-axis and remove superfluous axis elements
  theme(axis.text.x = element_text(angle = 45, 
                                   hjust = 1, vjust = 1))+
    labs(subtitle="Population Totals", 
       y="Population in Millions", 
       x="Area", 
       title="Rural vs Urban", 
       caption = "Source: midwest")+
    theme(legend.title = element_blank())

layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))
hist(wt)
hist(mpg)
hist(disp)

```





Because our focus is on how populations and sparsity of residents may play a part in how food deserts form, we can explore the population densities in both Rural and Urban areas by State.
```{r}
###RURAL POP DENSITY###
  

pop_density_rural <- county_char %>% 
  group_by(State, Metro2013) %>%
  filter(Metro2013 == 'Rural') %>% 
  mutate(area = sum(LandAreaSQMiles2010)) %>% 
  mutate(total_pop = sum(TotalPopEst2011)) %>% 
  mutate(density = sum(total_pop/area)) %>% 
  select(State, area, Metro2013, total_pop, density)

pop_density_rural <- unique(pop_density_rural)


ggrural <- ggplot(pop_density_rural, aes(x=area, y=(total_pop/1000000))) + 
  geom_point(aes(col=State, size=density)) + 
  geom_text(aes(label=ifelse(area>100000,as.character(State),'')),hjust=0,vjust=0) +
  geom_text(aes(label=ifelse(total_pop>1500000,as.character(State),'')),hjust=0,vjust=0) +
  geom_smooth(method="loess", se=F) + 
  #xlim(c(0, 0.1)) + 
  #ylim(c(0, 500000)) + 
  labs(subtitle="Area Vs Population", 
       y="Population Totals in Millions", 
       x="Area Totals (Sq Miles)", 
       title="Rural Population Density", 
       caption = "Source: midwest")


#########################

###URBAN POP DENSITY###

pop_density_urban <- county_char %>% 
  group_by(State, Metro2013) %>%
  filter(Metro2013 == 'Urban') %>% 
  mutate(area = sum(LandAreaSQMiles2010)) %>% 
  mutate(total_pop = sum(TotalPopEst2011)) %>% 
  mutate(density = sum(total_pop/area)) %>% 
  select(State, area, Metro2013, total_pop, density)

pop_density_urban <- unique(pop_density_urban)

ggurban <- ggplot(pop_density_urban, aes(x=area, y=(total_pop/1000000))) + 
  geom_point(aes(col=State, size=density)) + 
  geom_text(aes(label=ifelse(area>25000,as.character(State),'')),hjust=0,vjust=0) +
  geom_text(aes(label=ifelse(total_pop>15000000,as.character(State),'')),hjust=0,vjust=0) +
  geom_smooth(method="loess", se=F) + 
  #xlim(c(0, 0.1)) + 
  #ylim(c(0, 500000)) + 
  labs(subtitle="Area Vs Population", 
       y="Population in Millions", 
       x="Area Totals (Sq Miles)", 
       title="Urban Population Density", 
       caption = "Source: midwest")


#########################


###OVERALL POP DENSITY###

pop_density <- county_char %>% 
  group_by(State, Metro2013) %>%
  #filter(Metro2013 == 'Urban') %>% 
  mutate(area = (LandAreaSQMiles2010)) %>% 
  mutate(total_pop = (TotalPopEst2011)) %>% 
  mutate(density = (total_pop/area)) %>% 
  select(State, area, Metro2013, total_pop, density)

pop_density <- unique(pop_density)

gg <- ggplot(pop_density, aes(x=area, y=(total_pop/1000000))) + 
  geom_point(aes(col=Metro2013, size=density)) + 
  geom_text(aes(label=ifelse(area>150000,as.character(State),'')),hjust=0,vjust=0) +
  geom_text(aes(label=ifelse(total_pop>15000000,as.character(State),'')),hjust=0,vjust=0) +
  geom_smooth(method="loess", se=F) + 
  #xlim(c(0, 0.1)) + 
  #ylim(c(0, 500000)) + 
  labs(subtitle="Urban vs Rural by State Total", 
       y="Population in Millions", 
       x="Area Totals (Sq Miles)", 
       title="Population Density", 
       caption = "Source: midwest")


plot(ggrural)
plot(ggurban)
plot(gg)

```






#First we want to explore how some of our metrucs relate to Rural areas:

```{r}

names <- c(1,
           4:5,
           8:10,
           14,
           53:57)
county_char[,names] <- lapply(county_char[,names] , factor)

metro_comp <- county_char %>% group_by(Metro2013) %>% 
  summarise_if(is.numeric, mean, na.rm = TRUE)

Rural <- metro_comp %>% 
  filter(Metro2013 == "Rural")

Urban <- metro_comp %>% 
  filter(Metro2013 == "Urban")

#How have populations in Urban and Rural areas changed between 2000 and 2010

ggplot(metro_comp) +
  geom_bar(aes(x = Metro2013, y = PopChangeRate0010, fill = Metro2013),
           stat = "identity", position = "dodge") +
    labs(subtitle="Urban vs Rural Areas", 
       y="Percent Change", 
       x="County Type", 
       title="Population Change 2000 - 2010", 
       caption = "Source: midwest")+
  theme(legend.title = element_blank())


```
```{r}
metro_melt <- melt(metro_comp)

#################WORK TYPES########################
melt_work_types <- metro_melt[65:84,]

ggplot(melt_work_types) +
  # add bar for each discipline colored by gender
  geom_bar(aes(x = variable, y = value, fill = Metro2013),
           stat = "identity", position = "dodge") +
  # name axes and remove gap between bars and y-axis
  #scale_y_continuous("Value", expand = c(0, 0)) +
  #scale_x_discrete("variable") +
  scale_fill_manual(values = c("#468189", "#9DBEBB")) +
  # remove grey theme
  theme_classic(base_size = 18) +
  # rotate x-axis and remove superfluous axis elements
  theme(axis.text.x = element_text(angle = 45, 
                                   hjust = 1, vjust = 1)) +
      labs(subtitle="Urban vs Rural", 
       y="Percent Employment", 
       x="Employment Type", 
       title="Employment Type", 
       caption = "Source: midwest")+
  theme(legend.title = element_blank())
```

#########################################



```{r}
#################EDUCATION TYPES########################
melt_edu_types <- metro_melt[47:56,]


ggplot(melt_edu_types) +
  # add bar for each discipline colored by gender
  geom_bar(aes(x = variable, y = value, fill = Metro2013),
           stat = "identity", position = "dodge") +
  # name axes and remove gap between bars and y-axis
  #scale_y_continuous("Percent", expand = c(0, 0)) +
  #scale_x_discrete("Education Level") +
  scale_fill_manual(values = c("#468189", "#9DBEBB")) +
  # remove grey theme
  theme_classic(base_size = 18) +
  # rotate x-axis and remove superfluous axis elements
  theme(axis.text.x = element_text(angle = 45, 
                                   hjust = 1, vjust = 1)) +
      labs(subtitle="Urban vs Rural", 
       y="Percent Attainment", 
       x="Education Level", 
       title="Education Attainment Level", 
       caption = "Source: midwest")+
  theme(legend.title = element_blank())

#########################################
```
````{r}
melt_com_types <- metro_melt[9:20,]

ggplot(melt_com_types) +
  # add bar for each discipline colored by gender
  geom_bar(aes(x = variable, y = value, fill = Metro2013),
           stat = "identity", position = "dodge") +
  # name axes and remove gap between bars and y-axis
  #scale_y_continuous("Percent", expand = c(0, 0)) +
  #scale_x_discrete("Education Level") +
  scale_fill_manual(values = c("#468189", "#9DBEBB")) +
  # remove grey theme
  theme_classic(base_size = 18) +
  # rotate x-axis and remove superfluous axis elements
  theme(axis.text.x = element_text(angle = 45, 
                                   hjust = 1, vjust = 1)) +
    labs(subtitle="Urban vs Rural", 
       y="Percent of County", 
       x="Identity", 
       title="Ethnic Make-UP", 
       caption = "Source: midwest")+
  theme(legend.title = element_blank())
#########################################
```

```{r}
melt_pov_types <- metro_melt[57:64,]

ggplot(melt_pov_types) +
  # add bar for each discipline colored by gender
  geom_bar(aes(x = variable, y = value, fill = Metro2013),
           stat = "identity", position = "dodge") +
  # name axes and remove gap between bars and y-axis
  #scale_y_continuous("Percent", expand = c(0, 0)) +
  #scale_x_discrete("Education Level") +
  scale_fill_manual(values = c("#468189", "#9DBEBB")) +
  # remove grey theme
  theme_classic(base_size = 18) +
  theme(axis.text.x = element_text(angle = 45, 
                                   hjust = 1, vjust = 1)) +
  labs(subtitle="Urban vs Rural", 
       y="Poverty Rate", 
       x="Poverty Type", 
       title="Poverty Levels", 
       caption = "Source: midwest")+
  theme(legend.title = element_blank())


#########################################
```
```{r}
melt_income_types <- metro_melt[35:36,]

ggplot(melt_income_types) +
  # add bar for each discipline colored by gender
  geom_bar(aes(x = variable, y = value, fill = Metro2013),
           stat = "identity", position = "dodge") +
  # name axes and remove gap between bars and y-axis
  #scale_y_continuous("HH Income", expand = c(0, 0)) +
  #scale_x_discrete("Location") +
  scale_fill_manual(values = c("#468189", "#9DBEBB")) +
  # remove grey theme
  theme_classic(base_size = 18) +
  # rotate x-axis and remove superfluous axis elements
  theme(axis.text.x = element_text(angle = 0, 
                                   hjust = .5, vjust = 0),
        axis.line = element_blank(),
        axis.ticks.x = element_blank()) +
  labs(subtitle="Urban vs Rural", 
       y="Household Income (USD)", 
       x="", 
       title="Household Income", 
       caption = "Source: midwest")+
  theme(legend.title = element_blank())

```
```{r}
#########################################

# Basic scatter plot
# Change the point size, and shape

ggplot(county_char, aes(x=Pov.All, y=HSOnly, color=Metro2013)) +
  geom_point()+
labs(subtitle="Urban vs Rural", 
       y="% High School Diploma Only", 
       x="Poverty Rate", 
       title="High School Diploma by Poverty Rate", 
       caption = "Source: midwest")+
  theme(legend.title = element_blank())
##########################3333
```


```{r}
low_edu <- county_char %>% group_by(Metro2013) %>% 
  summarise(Low.Education = sum(as.numeric(Low.Education)))

low_emp <- county_char %>% group_by(Metro2013) %>% 
  summarise(Low.Employment = sum(as.numeric(Low.Employment)))


low_e <- merge(low_edu, low_emp, by = "Metro2013")

low_e <- melt(low_e)


ggplot(low_e) +
  # add bar for each discipline colored by gender
  geom_bar(aes(x = variable, y = value, fill = Metro2013),
           stat = "identity", position = "dodge") +
  # name axes and remove gap between bars and y-axis
  #scale_y_continuous("County Count", expand = c(0, 0)) +
  #scale_x_discrete("Variables") +
  scale_fill_manual(values = c("#468189", "#9DBEBB")) +
  # remove grey theme
  theme_classic(base_size = 18) +
  # rotate x-axis and remove superfluous axis elements
  theme(axis.text.x = element_text(angle = 0, 
                                   hjust = .5, vjust = 0),
        axis.line = element_blank(),
        axis.ticks.x = element_blank())+
  labs(subtitle="Urban vs Rural", 
       y="Number of Counties", 
       x="", 
       title="Low Education and Employment", 
       caption = "Source: midwest")+
  theme(legend.title = element_blank())

########################
```

#EXPLORE
#FOOD DESERT EXPLORATION
#WHAT VARIABLES ARE MOST HIHGLY CORRELATED WITH BEING A FOOD DESERT

#MODEL 1


#MODEL PREPARATION


Currently, we have 63 total columns in our dataset. Our final columns is out Response, and the first 3 are identification attributes. Therefore, since we have such a large number of predictors, we can create a quick formula for our models instead of typing each variable into the glm formula(*x1, x2, x3....xn*).

```{r}
vars <- paste("",colnames(final[,4:62]),sep="")
fla <- paste("Desert ~", paste(vars, collapse="+"))
as.formula(fla)
```

Our model includes a variety of scaled factors, which, if left untreated, woudld inaccurately skew the results, therefore, we need to scale all values. A simple step to do this includes scaling all numeric factors in one subest, and then rejoining them to the factor variables.

```{r}
z_outfd <- dplyr::select_if(final, is.numeric) %>% 
  psycho::standardize()

nonnum <- final %>% select_if(negate(is.numeric))


z_outfd <- as.data.frame(z_outfd)

nonnum <- as.data.frame(nonnum)
scaled_final <- cbind(nonnum, z_outfd)
```


#RANDOM FOREST FEAUTRE EXPLORATION 
#FEATURE ENGINEERING

As mentioned previously, we 59 predictors in our model, and it is likely that some impact our response more than others, and that some may not impact the response at all. We can therefore use the Random Forest Variable Importance function to identify the variace of each variable on our model and adjust ot accordingly.

```{r}
set.seed(123)
fit_rf = randomForest(as.formula(fla), data=scaled_final, importance = TRUE, na.action = na.roughfix) #fit the model using the formula created above
```

# Create an importance based on mean decreasing gini

Identify the importance values (decrsing gini)
```{r}
importance(fit_rf)
varImp(fit_rf)
impplot <- varImpPlot(fit_rf)
```

Results of our model including accuracy using all predictors as a baseline.
```{r}
fit_rf
```

#^^^^THIS IS FOR VARIABLE IMPORTANCE __ NOT MAKING ANY PREDICTIONS

```{r}
plot(randomForest(as.formula(fla), data=scaled_final, keep.forest=FALSE, ntree=100), log="y")
```

Since the purpose of our study is to identufy liklehood or probability of becoming a food desert, we want to build a baseline probit regression using all predictors. For this process, we are going to use the sparklyr and dplyr packages, so we need to copy out final, scaled dataset back to Spark for processing.

```{r}
#copy the scaled data frameback to spark
scaled_final = sdf_copy_to(sc, scaled_final, "scaled_final", overwrite = TRUE)
```

To check for accuracy, we are going to radnomly split our data into partitions with a 75/25 split. Note that we are setting a seed for reproducbility.
```{r}
set.seed(123)
partitions <-  scaled_final %>%
  sdf_random_split(training = 0.75, test = 0.25, seed = 123)

scaled_final_training <- partitions$training
scaled_final_test <- partitions$test
```

We are using the ml_logistic_regression algorithm from spark, where we can use a binomial setting to return probabilities.

```{r}
#fit the first probit model
mysparkprobit <- scaled_final_training %>% 
  ml_logistic_regression(fla, data = scaled_final, family = "binomial",  maxit = 100)

#make predictions
pred <- ml_predict(mysparkprobit, scaled_final_test,  type="response")

#AREA UNDER THE ROC
ml_binary_classification_evaluator(pred)
```

With our baseline model created, we can get an accuracy measure as well as plot some of our results for better interpretation and comparison for future models.
```{r}
#view predictions in liklihood of being a food desert order
predsvis <- pred %>% arrange(desc(prediction)) %>% 
  select(State, County, FIPS, Desert, prediction, probability_0, probability_1, METRO) %>% 
  collect()


predsvis

```
write.csv(predsvis, file = "C:/Users/blume/Desktop/preds.csv")

Note that because we are using all the predictors in our model it is likely that it is overfit. We will want to tweak our model, however, we also want to set baselines for performance and accuracy against some other common algorithms.

As a check on our first Feature Importance algorithm, we are going to run a seprate model for validation. This is a spark decision tree that will identify, rank, and visualize features based off impact on the response.

```{r}
#SPARK FEATURE IMPORTANCE
set.seed(123)
spark_decision_tree <- ml_decision_tree(scaled_final_training, fla) #train model

spark_decision_tree #view model

feat_importance <- ml_tree_feature_importance(spark_decision_tree) #variable stores results

feat_importance #view results
```

We are going to create 4 separate models and compare the accuracy against not only each other, but against a random chance baseline.

```{r}
## Logistic Regression
ml_log <- ml_logistic_regression(scaled_final_training, fla)

## Decision Tree
ml_dt <- ml_decision_tree(scaled_final_training, fla)

## Random Forest
ml_rf <- ml_random_forest(scaled_final_training, fla)

## Gradient Boosted Tree
ml_gbt <- ml_gradient_boosted_trees(scaled_final_training, fla)



#Score the test data with the trained models.

ml_models <- list(
  "Logistic" = ml_log,
  "Decision Tree" = ml_dt,
  "Random Forest" = ml_rf,
  "Gradient Boosted Trees" = ml_gbt
)

# Create a function for scoring
score_test_data <- function(model, data=scaled_final_test){
  pred <- ml_predict(model, data)
  select(pred, Desert, prediction)
}

# Score all the models
ml_score <- lapply(ml_models, score_test_data)


#ml_score

```
###Model lift
Lift compares how well the model predicts food deserts compared to random guessing. Use the function below to estimate model lift for each scored decile in the test data. The lift chart suggests that the tree models (random forest, gradient boosted trees, or the decision tree) will provide the best prediction.

```{r}
# Lift function
calculate_lift <- function(scored_data) {
  scored_data %>%
    mutate(bin = ntile(desc(prediction), 10)) %>% 
    group_by(bin) %>% 
    summarize(count = sum(Desert)) %>% 
    mutate(prop = count / sum(count)) %>% 
    arrange(bin) %>% 
    mutate(prop = cumsum(prop)) %>% 
    select(-count) %>% 
    collect() %>% 
    as.data.frame()
}

# Initialize results
ml_gains <- data.frame(bin = 1:10, prop = seq(0, 1, len = 10), model = "Base")

# Calculate lift
for(i in names(ml_score)){
  ml_gains <- ml_score[[i]] %>%
    calculate_lift %>%
    mutate(model = i) %>%
    rbind(ml_gains, .)
}

# Plot results
p_all_vars <- ggplot(ml_gains, aes(x = bin, y = prop, colour = model)) +
  geom_point() + geom_line() +
  ggtitle("Lift Chart for Predicting Food Deserts - Test Data Set") + 
  xlab("") + ylab("")

p_all_vars
```

##GRADIENT BOOSTED TREE

gbt_pred <- ml_predict(ml_gbt, scaled_final_test)

gbt_pred %>%  select(State, County, prediction, Desert)



In an effort to improve accuracy, we will only use the predictors that the Random Forest Variable Importance test found to be impactful. This means we need to create a new formula:

```{r}
feat_importance #view feature scores from decision tree feature importance

#updated formula
vars1 = feat_importance$feature[1:22]
vars1
vars1 <- replace(vars1, vars1=="SNAP_OAPP_0", "SNAP_OAPP")
vars1 <- replace(vars1, vars1=="SNAP_OAPP_1", "SNAP_OAPP")

fla1 <- paste("Desert ~", paste(vars1, collapse="+"))
as.formula(fla1)
```




##############################UPDATED FEATURE MODELS##########################################
##############################################################################################
##############################################################################################

```{r}
## Logistic Regression
ml_log_ftr <- ml_logistic_regression(scaled_final_training, fla1)

## Decision Tree
ml_dt_ftr <- ml_decision_tree(scaled_final_training, fla1)

## Random Forest
ml_rf_ftr <- ml_random_forest(scaled_final_training, fla1)

## Gradient Boosted Tree
ml_gbt_ftr <- ml_gradient_boosted_trees(scaled_final_training, fla1)



#Score the test data with the trained models.

ml_models_ftr <- list(
  "Logistic" = ml_log_ftr,
  "Decision Tree" = ml_dt_ftr,
  "Random Forest" = ml_rf_ftr,
  "Gradient Boosted Trees" = ml_gbt_ftr
)

# Create a function for scoring
score_test_data_ftr <- function(model, data=scaled_final_test){
  pred <- ml_predict(model, data)
  select(pred, Desert, prediction)
}

# Score all the models
ml_score_ftr <- lapply(ml_models_ftr, score_test_data_ftr)


#ml_score_ftr

```
###Model lift
Lift compares how well the model predicts food deserts compared to random guessing. Use the function below to estimate model lift for each scored decile in the test data. The lift chart suggests that the tree models (random forest, gradient boosted trees, or the decision tree) will provide the best prediction.

```{r}
# Lift function
calculate_lift <- function(scored_data) {
  scored_data %>%
    mutate(bin = ntile(desc(prediction), 10)) %>% 
    group_by(bin) %>% 
    summarize(count = sum(Desert)) %>% 
    mutate(prop = count / sum(count)) %>% 
    arrange(bin) %>% 
    mutate(prop = cumsum(prop)) %>% 
    select(-count) %>% 
    collect() %>% 
    as.data.frame()
}

# Initialize results
ml_gains_ftr <- data.frame(bin = 1:10, prop = seq(0, 1, len = 10), model = "Base")

# Calculate lift
for(i in names(ml_score_ftr)){
  ml_gains_ftr <- ml_score_ftr[[i]] %>%
    calculate_lift %>%
    mutate(model = i) %>%
    rbind(ml_gains_ftr, .)
}

# Plot results
p_var_imp <- ggplot(ml_gains_ftr, aes(x = bin, y = prop, colour = model)) +
  geom_point() + geom_line() +
  ggtitle("Lift Chart for Predicting Food Deserts - Test Data Set") + 
  xlab("") + ylab("")

p_var_imp
```

#############################################################################################
#############################################################################################
#############################################################################################









##############DISCUSS RESULTS AND PICK A MODEL###########################
#########################################################################

As our inital suggestion was to create a model that offers a probability prediction, we want to create another probit model that uses our updated variable set:

```{r}

##updated probit model

updated_probit <- scaled_final_training %>% 
  ml_logistic_regression(fla1, data = scaled_final, family = "binomial",  maxit = 100)

#make predictions
updated_probit_pred <- ml_predict(updated_probit, scaled_final_test)

#updated_probit_pred

#AREA UNDER THE ROC
ml_binary_classification_evaluator(updated_probit_pred)

```
View the resulst

```{r}
#view predictions in liklihood of being a food desert order
updated_predsvis <- updated_probit_pred %>% arrange(desc(probability_1)) %>%
  collect()

updated_predsvis
```
write.csv(predsvis, file = "C:/Users/blume/Desktop/preds.csv")


###############EXPLAIN WHICH MODEL WE ARE GOING TO USE FOR OUR FINAL RESULTS###################


#________________#_________________#_________________#
#________________#_________________#_________________#
#________________#_________________#_________________#


With our model selected, we want to use it to score the likely food deserts based off of the most recent complete census data (2015).

#IMPORT 2015 TESTING DATA

Similar to our inital dataset, we need to import, pre-process and clean the final dataset.

```{r}
access2015 <- spark_read_csv(sc, name = 'access2015', path = 'C:/Users/blume/Desktop/practicum/data/2015/access2015.csv', header = TRUE, delimeter = ',', stringsAsFactors = FALSE)
assistance2015 <- spark_read_csv(sc, name = 'assistance2015', path = 'C:/Users/blume/Desktop/practicum/data/2015/assistance2015.csv', header = TRUE, delimeter = ',', stringsAsFactors = FALSE)
health2015 <- spark_read_csv(sc, name = 'health2015', path = 'C:/Users/blume/Desktop/practicum/data/2015/health2015.csv', header = TRUE, delimeter = ',', stringsAsFactors = FALSE)
insecurity2015 <- spark_read_csv(sc, name = 'insecurity2015', path = 'C:/Users/blume/Desktop/practicum/data/2015/insecurity2015.csv', header = TRUE, delimeter = ',', stringsAsFactors = FALSE)
local2015 <- spark_read_csv(sc, name = 'local2015', path = 'C:/Users/blume/Desktop/practicum/data/2015/local2015.csv', header = TRUE, delimeter = ',', stringsAsFactors = FALSE)
restaurants2015 <- spark_read_csv(sc, name = 'restaurants2015', path = 'C:/Users/blume/Desktop/practicum/data/2015/restaurants2015.csv', header = TRUE, delimeter = ',', stringsAsFactors = FALSE)
socioeconomic2015 <- spark_read_csv(sc, name = 'socioeconomic2015', path = 'C:/Users/blume/Desktop/practicum/data/2015/socioeconomic2015.csv', header = TRUE, delimeter = ',', stringsAsFactors = FALSE)
store2015 <- spark_read_csv(sc, name = 'store2015', path = 'C:/Users/blume/Desktop/practicum/data/2015/store2015.csv', header = TRUE, delimeter = ',', stringsAsFactors = FALSE)

output2015 <- Reduce(function(...) merge(..., all=TRUE), list(access2015, assistance2015, health2015, insecurity2015, local2015, restaurants2015, socioeconomic2015, store2015))



#remove any rows where state > 2 charachters

output2015 <- subset(output2015, nchar(as.character(State)) <= 2)



output2015$FIPS <- as.character(output2015$FIPS)
output2015$METRO <- as.factor(output2015$METRO)
output2015$SNAP_OAPP <- as.factor(output2015$SNAP_OAPP)
output2015$SNAP_FACEWAIVER <- as.factor(output2015$SNAP_FACEWAIVER)
output2015$SNAP_VEHEXCL <- as.factor(output2015$SNAP_VEHEXCL)
output2015$SNAP_BBCE <- as.factor(output2015$SNAP_BBCE)
output2015$SNAP_REPORTSIMPLE <- as.factor(output2015$SNAP_REPORTSIMPLE)
output2015$FOODHUB <- as.factor(output2015$FOODHUB)
output2015$FARM_TO_SCHOOL <- as.factor(output2015$FARM_TO_SCHOOL)
output2015$PERPOV <- as.factor(output2015$PERPOV)
output2015$PERCHLDPOV <- as.factor(output2015$PERCHLDPOV)

output2015$MEDHHINC <- as.numeric(output2015$MEDHHINC)
output2015$CHILDPOVRATE <- as.numeric(output2015$CHILDPOVRATE)
output2015$POVRATE <- as.numeric(output2015$POVRATE)



output2015[is.na(output2015)] <- 0

output2015 <- na.omit(output2015)

vis_miss(output2015)
output2015 <- na.omit(output2015)



#scale continuous variables

outfd2015 <- dplyr::select_if(output2015, is.numeric) %>% 
  psycho::standardize()

nonnum2015 <- output2015 %>% select_if(negate(is.numeric))

finaloutput2015 <- cbind(nonnum2015, outfd2015)


#copy to Spark

finaloutput2015 = sdf_copy_to(sc, finaloutput2015, "finaloutput2015", overwrite = TRUE)

```

With our data processed and cleaned, we can run it through our model to determine where we may find food deserts based of 2015 data

```{r}
#final prediction

final_probit <- scaled_final %>% 
  ml_logistic_regression(fla1, data = scaled_final, family = "binomial",  maxit = 100)

final_probit_pred <- ml_predict(final_probit, finaloutput2015)

#separate the prediction information from the demographic info

final_vis <- final_probit_pred %>% 
  select(FIPS, State, County, prediction, probability_1, probability_0)

#final_probit_pred


final_vis

tab_out <- merge(final_vis, county_char)

write.csv(tab_out, "C:/Users/blume/Desktop/2015_preds.csv")
```

#________________#_________________#_________________#
#________________#_________________#_________________#
#________________#_________________#_________________#

RESULTS